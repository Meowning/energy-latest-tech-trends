7/30 최신기술동향 시작
리드미에 뭐 만들지 정리하고, 서버컴 성능 생각해서 약간 가볍고 빠른 프레임워크, 기술 스택으로만 구성함. 글고 크롤링 가능한 사이트 정리해둠

7/31
어제 정한 스택으로 프론트/백 initial setting 하고
껍데기를 구상하고 필요한 기능을 만드는 게 편할 거 같아서 프론트부터 먼저 함.
근데 할 일도 많고 공부할 시간이 필요해서 많이는 못함...

8/1 ~ 8/3
회사 휴일이라 나도 같이 코딩 휴일함...

8/4
난 이때 scroll-snap이란 용어를 처음 알았다...
암튼 저번주에 공부한 거랑 웹프에서 리액트 다뤘던 기억을 떠올려 Svelte 기반으로 만듦...
JS 기반인거 빼고 약간 다르지만 라우팅 짱
글고 리드미 좀 더 다듬음

8/5
오늘 바빠서 코딩 많이 못함...
RPA 서비스만 하기엔 난이도가 너무 쉬운 거 같아서 OCR + 추출/생성 요약 넣음
약간 모밋 할때 생각난다 ㅎ

8/6
OCR이 말을 안듣는다... 지금 이미지 기반이라서, OCR 전처리에서 안 걸러진 디자인들이 이상하게 인식되고 있음...
그래서 텍스트 기반 pdf면 걍 가져오고, 아니면 OCR 하기로... 정확도는 높아지겠지
글고 하루죙일 전처리만 함. 형태소 분석, 불용어 제거... 아마 추출요약은 키워드로 가지 않을까 싶은데
글고 생성요약은 CPU에서 잘 돌아가는 경량화된 T5 계열... 일단 더 조사해봐야 할 거 같음
일단 추출요약은 단어 빈도 분석으로 하는걸로...!!

8/7
오늘 엄청 뜯어고쳤다...
아무래도 이게 에너지, 기술, 원자력 이런 쪽 도메인이다 보니깐
단어 빈도 분석으로 하니깐, 불용어가 너무 많이 나옴...
지금 샘플로 돌리고 있는 pdf에서 다 걸러진다 해도, 다른 데에서 일일이 다 걸러낼 수가 없으니깐
추출요약 방법을 변경함.

기존에는 형태소로 다 짜르고 단어의 빈도를 확인해서 상위 n개 띄우는 방식이였다면,
좀 더 의미있는 결과를 얻기 위해 이런 식으로 바꿈
① 문장별로 벡터 임베딩(SBERT)
② n개 군집으로 클러스터링
③ 군집별로 중심 임베딩 계산하고 가장 가까운 대표 문장 선택 (거리 계산)

그래서 전처리 방식도 바꼈는데, 기존처럼 형태소 분석을 하지 않고,
- 띄어쓰기 정리 (병렬 배치된 본문들의 경우 줄바꿈이 애매모호해서, 걍 공백 다 없애고 딥러닝 모델로 띄어쓰기 다시 하기)
- 전화번호나 의미없는 숫자 지우기
- 이메일, 웹사이트 주소 지우기
- 문장부호 제외 특수문자 지우기
- 노이즈 (의미없는 문장) 필터링
- 글고 문장 단위로 나눔
    - 이제 보니깐 소숫점도 문장 취급 당해버리네. 이건 수정해야지

이런 식으로 하면 불용어가 많은 키워드보다 의미있는 결과가 나올거라 예상됨
일단 이렇게 해보고, 평가기준이 있으려나... 없겠지... 괜찮은거 같으면 계속 이렇게 쓸거임...
생성형 요약은 또 언제 하지... 일단 문장이 너무 길어서 요약이 안되니깐 그거부터 해결해보자

8/8
내일 ADsP 시험임... 코딩할 시간 없어...

8/11
아 기억났다!!! 지금 추출요약에 들어가는 데이터가 이상하게 들어갔던 거 같은데, 한번 코드리뷰 해봐야겠음
일단 OCR 할때 kss 문장 분리 과정을 이미 했는데, 추출 요약 때 띄어쓰기까지 한 결과만
넘어가서 문장 분리를 한번 더 하는게 보임 (비효율적)
그래서 일단 고쳤고... 플래그를 써야되나? 싶었는데 걍 할려고...
이제 생성형 요약 부분을, 지금은 한 문장으로 되어있는데 부분부분 내서 잘라서 넣을거임
요약 기법은 크게 세가지: Stuff, Refine, Map Reduce 방식
일단 나온 문장이 짧으면 Stuff 방식, 길면 Refine 방식을 채택하는 함수 만들어서 해봐야겠음


글고 첨에 전처리때 맞춤법검사기(네이버)를 넣어봤는데 성능이 안 좋아서 뺐었음
근데 좀 더 찾아보니깐 맞춤법검사기 중에 교차검증(다음+부산대학교) 가능한 소스가 있음
일단 파이썬 라이브러리는 아니고 nodejs로 작동하는 것 같은데, 한번 좀 더 알아봐야겠음...
Docker 서비스로 따로 빼는거 고려도 해야겠음.. 후처리도 필요하니깐

일단 전처리 로직을 다시 생각해볼려고 지피티랑 대화 좀 나눠봄...
왜냐면 아무리 텍스트 기반이여도 문서마다 한 단/두 단/세 단 이상 이렇게 나눠지기도 하고
본문이 없는 페이지인 경우도 있음
그래서 띄어쓰기 다 붙이기 -> spacing -> kss 해도 뭔가 품질이 그닥... 그닥임
원래의 품질을 살리면서도 이쁘게 전처리된 무언가가 필요했음

그래서 내린 결론은? 단 같은 경우는 좌표 기반으로 레이아웃이 n단이다 인지를 하고,
칼럼별로 재정렬시킴(텍스트에선 별 상관없겠지만, 이미지에선 그냥 한 문장 취급되니깐)
그리고 줄바꿈 뒤어있는거 문장부호 없이 끝나면 합치고, 줄바꿈하고,
pykospacing, kss 쓰고.

그리고 생성형 요약 t5-small 기반 한국어 파인튜닝 모델 찾아서 쓰려고 했는데,
Hugging Face에서 모델 다운로드가 잘 안됨...
ㄴ 아니 오프라인 모드가 왜 켜져있어?? 어쩐지 안되더라....
아니네 그거 아니고도 코드스페이스에서 허깅페이스를 막아둠... 왜..??

8/12
허깅페이스 다운로드 이슈는 그냥 인터널서버 에러였던 듯함.. 잘 됨..
일단 한번 다운로드 성공하면 오프라인에서 캐시된거 쓸 수 있도록
그리고 그 단락별로 나눠서 하는거, 스페이스 모듈 때문에 오히려 원래의 의미가 틀어져서 스페이스 모듈을 빼기로 결정.
그렇게 테스트 해봤는데 애초에 단락별로 나눠서 하는 과정에서 단락이 제대로 합쳐지지 않는 듯한 양상을 보여 파이프라인을 예전으로 되돌림 ㅠㅠ
(OCR - 전처리 - 스페이싱 - KSS - 추출요약 - 생성요약)
글고 최대한 본문만 AI 요약으로 넘기기 위해, 앞쪽 내용에서 키워드(목차, TEL, FAX 등)로
다 잘라버리고 마지막 마침표 이후로는 다 날려버림. 그나마 괜찮지 않을까?
ㄴ 생성 요약은 괜찮게 잘 나오는 거 같은데, 추출요약이 많이 이상하게 나온다.

```{
    extractive_summary":"[111] 특히, 최근 글로벌 빅테크 기업과 산업계는 장기 무탄소 전력공급을 확보하기 위해 자가발전, 온 사이트 오프사이트 PPA 등 다양한 방식으로 SMR 전력 도입을 추진하고 있다.\n
    [221] Maxim, Es quire, Outback, Toma hawk, Droptop, Starbucks.\n
    [225] 그건 누구나 안다고 여기고 찾아보지 않는다."
}```

첫번째는 괜찮게 나온 것 같은데, 두번째랑 세번째는 왜 저렇게 나오지?
이래서 사람들이 추출요약 안 쓰나 보다. 아니면 그냥 내가 잘못 코딩한걸지도 ㅎ

생성요약은 CPU에서도 빨리 돌아가고 진짜 괜찮게 나와서, 기존의 한줄 출력을 세줄로 늘렸다.
일단 T5가 text-to-text 모델이고, 지금 쓰는건 한국어로 파인튜닝된 모델인데,
프롬프트도 한글로 주는게 좋을까? 일단 영어로 줬다. 해보고 바꿔야징 ㅎ

---
그리고 양질의 요약을 위해, 디코딩을 좀 만졌다.
얘도 일단 트랜스포머 기반 모델이니깐! encoder-decoder 구조.
너무 짧게 생성되는 것을 막으려고 토큰의 최소 수를 50으로 설정했다.
그리고 max token은... 일단 테스트해보고, 너무 길게 나오면 제한을 둘 예정이다,,,
시간을 아끼면서 품질이 떨어지는 걸 방지하기 위해 떨어지는 추세면 얼리스탑 넣었당.

일단 생성요약 구조가 어떻게 되나면 3문장의 한국어 요약을 생성(하려고 했음..)
입력이 짧으면 Stuff 방식, 입력이 길면 Refine(순차보강) 방식.
흐름은 전처리 -> 문장 병합 -> Stuff/Refine(청크별로 너무 길면 짜름) -> 3문장 맞추기
이런 식임. 근데 왜 똑같은 문장만 세번 뜨냐? 원래 한줄짜린데 일부러 늘려서 저렇게 나오나
---

**아 실패했던 n단으로 나눠서 하는 거... 뭔가 컴퓨터비전으로 특징 추출해서 본문만 읽을 수 있게 할 수 있지 않을까??** 라는 생각이 갑자기 든다.

> 엇 재미있는 프로젝트가 생각이 났다. 프로야구 우천취소 예측 프로젝트 어떰??? 오늘 야구보러 가는데 우천취소 되면 화딱지 날 것 같음...

8/13
일단 어제 생성 요약을 좀 고쳐봤다.
저렇게 청크별로 보되 청크 내 토큰이 너무 길면 짜르는 방식은
확실히 작업속도를 많이 줄여주긴 하지만 전체 맥락을 참조할 수 없음
그렇다고 청크 내 내용을 안 줄이고 쌩으로 Refine하면
아무래도 너무 많아서 속도 느려지는 게 무시할 수 없을 정도임.

그래서 Refine 하기 전에 청크별로 추출요약을 한번 하면 줄어들려나 싶음
전처리 -> 청크별 추출요약 -> Refine -> 3문장
좀 느려지고 뭔가 잘 돌아가는 느낌이 나긴 하는데 (68페이지에 5분 정도), 여전히 3문장 중복 출력 중
해결 방법이 뭐가 있을까?
1. 프롬프트 강화, 2. 파라미터 조정, 3. 문장이 하나만 나온 경우 
1. 일단 프롬프트에 "각 문장은 반드시 다른 정보" 이런 느낌의 얘기를 두세번 했음
2. 디코더 파라미터 바꿔봄
```return dict(
        max_new_tokens=int(os.getenv("T5_MAX_NEW_TOKENS", "120")),
        min_new_tokens=int(os.getenv("T5_MIN_NEW_TOKENS", "45")),
        num_beams=int(os.getenv("T5_NUM_BEAMS", "2")),
        do_sample=False,
        use_cache=True,
        no_repeat_ngram_size=int(os.getenv("NO_REPEAT_NGRAM", "4")),
        repetition_penalty=float(os.getenv("REPETITION_PENALTY", "1.1")),
        early_stopping=True,
    )```

지금 디코더가 이런데, num_beams = 빔 서치라는 걸 우선 알아야 함
---
NLP의 텍스트 생성 방식 중에 우선 가장 기본은 그리디 서치임.
그리디 서치는 가장 확률이 높은 단어 하나만을 선택해 문장을 이어나가는 방식임.
자료구조나 알고리즘 때.. 아 알고리즘때 배우잖음 그리디.
이 방식은 당장 눈앞에 제일 좋아보이는 하나만 고르는 거라 장기적으로 볼 수 없어서,
어색하거나 의미가 맞지 않는 문장으로 이어질 수 있음
그리디 서치가 num_beams=1임.
beam은 후보 문장이고 num_beams는 각 단계에서 고려할 후보 문장의 개수를 지정해주는 파라미터임.

그래서 빔 서치라는 보완책을 쓰는거임.
대충 예상 가겠지만, 한 번에 num_beams만큼의 후보 문장들을 동시에 고려하면서 이어나감.
자세한 내용은 링크 첨부 (https://wb2x.tistory.com/89)
---

다시 정리해보자.
전처리가 끝난 문장은 생성 요약을 하기 위해 청크 단위로 나누게 된다.
청크 단위는 토큰(단어 단위보다 잘게 쪼갠 조각)의 개수를 기준으로 분리되는데,
여기서 그냥 뚝 하고 잘라버리면 문장이 끊어져버려 의미 단절의 확률이 높아짐.
그런데도 속도, 모델의 보정(T5 계열), 오버랩 방식 등이 있어 걍 씀 

- 문장 경계 맞춰서 분리하고, 청크 사이즈를 넘지 않는 선에서 문장 단위로 묶어 생성 + 오버랩
이런 방법도 있긴 한데, 어차피 오버랩 쓰면 이전 청크에서 끊기더라도 다음 청크에서 이전 핵심 내용은 다 오버랩돼서 걍 뚝 끊어도 상관없음!

그냥 내 감상인데, 맨날 백엔드, 인프라, 이런것만 하다가 실제로 AI 해보니깐 고려할 게 생각보다 굉장히 많은 것 같다.

AI로 뭘 할거냐에 따라(키워드추출? 요약?)서도 전처리 방법이 달라지고,
딥러닝 기반 모델 쓸때도 특성이 다 다르고 (예시로 내가 쓴 건 transformer 계열, sentence-to-sentence지만, TF-IDF, Word2Vec 등 목적에 따라 쓰임새가 다름)
직접 부딪히면서 배운 것도 많음.

트랜스포머 공부 정리!! 이따 스터디에도 써야징
---
트랜스포머 계열 모델은 인코더-디코더로 이루어져 있으면서 핵심 기술은 다음과 같음

# Multi-Head Self-Attention
: 다른 단어들과의 관계를 여러번 반복하여 학습
임베딩 -> 어텐션 스코어(각 단어와 모든 단어들의 관계) 계산
-> 소프트맥스(전체에 대해 확률 합을 1로 보정해줌) -> 가중합 (소프트맥스 확률 더함)

# Position-Wise Feed-Forward Networks
셀프어텐션 레이어의 출력을 받아 각 단어를 비선형적으로 변환하여 정보를 풍부하게 만듦

# Residual Connection and Layer Normalization
잔차연결(입력을 출력에 더함). 기울기 소실 문제를 완화하고 층 정규화를 수행하여 학습 가속화, 모델의 일반화 능력 향상

# Positional Encoding
입력 시퀀스의 순서를 인식하기 위해 위치 인코딩을 추가

시퀀스 처리 단계는 다음과 같음
1. 인코더
- 위치 인코딩, 입력 임베딩
- Multi-Head Self-Attention
- Position-Wise Feed-Forward Networks
- 잔차연결, 층 정규화
- 반복

2. 디코더
- 위치 인코딩, 입력 임베딩
- Masked Multi-Head Self-Attention (미래의 단어를 보지 못하도록 마스킹)
- Encoder-Decoder Attention (인코더의 출력을 참조하여 학습)
- Position-Wise Feed-Forward Networks

3. 출력 (소프트맥스를 통해 확률분포로 변환하여 다음 단어 예측)
---

아무튼 이래서 인코딩은 학습부인데, 내가 이미 한국어로 파인튜닝된 모델을 쓰고 있어서
내가 T5에 새로 학습시키지 않는 이상 내가 뭐 어떻게 못하고,
디코더의 파라미터를 어떻게 조절하느냐에 따라 출력의 결과가 달라진다는게 신기함.
프로젝트 하면서 ADsP 시험 보길 잘한 것 같다. (공부는 많이 안했지만...)
확실히 아는 게 늘어나니깐 내가 흡수할 수 있는 지식의 범위도 늘어난 느낌!

OCR + 요약은 이쯤에서 마무리하고, 요약 결과 몇개만 더 돌려보고 백엔드 작업에 들어갈 것이다. 속도는... 느리긴 하지만...ㄴ ㅏ중에 고칠래 제대로 돌아가니깐... 일단 끝끝!!!
일단 지금 간행물들 다 열어봐서 불용어? 본문이 아닌 부분? CLEAN_KEYWORD 다 적어놔야지

엇 아니다 후처리 안했다... 후처리는 굳이 안해도 되지만 사용자에게 제공하는 정보의 품질을 높이려면 하는 게 좋을 것 같음! 맞춤법검사기만 하면 될 것 같다. 부산대 그걸로...

일단 # TODO: 간행물 전부 열어봐서 CLEAN_KEYWORD 업데이트하기
# TODO: 후처리
# TODO: 백엔드 개발