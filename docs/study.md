# OCR, 인공지능 요약 스터디

## 광학 문자 인식 (Optical Character Recognition, OCR)
- 인쇄된 문서(스캔본 등)를 디지털 텍스트로 변환하는 기술
- 주로 스캔 이미지(PNG, JPG, TIFF)나 PDF에서 텍스트 영역을 인식

### 작동 방식
1. **이미지 분석**
- 스캐너 또는 카메라로 문서를 디지털 이미지(바이너리 데이터)로 변환
- 밝은 영역은 배경, 어두운 영역은 텍스트로 구분

2. **사전 분석 (전처리)**
- **이진화**:
  - **Otsu Thresholding**: 전역 임계값 자동 계산
  - **Adaptive Thresholding**: 영역별 임계값 적용, 조명 불균일 보정
- **이미지 품질 개선**: 텍스트 가장자리를 매끄럽게, 얼룩 제거, 노이즈 필터링(Gaussian, Median)
- **기울기 보정**: Hough Transform으로 텍스트 라인 각도 검출 후 회전
- **불필요 요소 제거**: 선, 상자, 로고, 여백 등 ROI(Region of Interest) 추출
- **다국어 처리**: 스크립트 자동 감지 후 해당 언어 모델 적용
- **해상도 조정**: 문서 OCR 권장 해상도 최소 300 DPI 이상

3. **텍스트 인식**
- **특징 추출**: 문자를 폐쇄 루프, 선 방향, 교차 등 구성 요소로 분해 후 매칭
- **패턴 일치**: 문자 이미지(글리프)를 저장된 글리프와 비교  
  (이미 알려진 글꼴일 때 정확도 높음)
- **딥러닝 방식**: CRNN, Vision Transformer, TrOCR 등으로 문맥·글자 동시 인식

4. **사후 처리 (후처리)**
- **오탈자 보정**: 사전 기반 맞춤법 교정(Hunspell, SymSpell 등)
- **띄어쓰기 복원**: PyKoSpacing, KIWI
- **형식 정규화**: 날짜(`2025-08-13`), 수치(`15.7 MW`) 표준화
- **출력**: 추출된 텍스트를 파일로 저장, 필요 시 원본 PDF와 병합
- 인식 실패 시 스캔 품질, 조명, 왜곡 여부 점검

---

## 인공지능 (Artificial Intelligence, AI)

### 개요
- **인공지능(AI)**: 인간의 학습, 추론, 문제 해결 능력을 컴퓨터가 모방·확장하는 기술
- **딥러닝(Deep Learning)**: 인공신경망(ANN)을 여러 층으로 쌓아 복잡한 패턴을 학습하는 기계학습의 한 분야
- 대규모 데이터와 고성능 연산 자원(GPU 등)을 활용하여 이미지 인식, 자연어 처리(NLP), 음성 인식, 추천 시스템 등 다양한 분야에서 사용

---

### 다층 퍼셉트론(MLP, Multi-Layer Perceptron)
- **구조**: 입력층 → 은닉층(여러 개) → 출력층, 각 층은 완전연결(fully connected)
- **역할**: 입력 특징을 비선형적으로 변환하여 복잡한 패턴 학습
- **NLP와의 관계**:
  - 과거: TF-IDF, Word2Vec 벡터를 MLP에 입력하여 분류/예측
  - 현재: Transformer 내부의 **Position-Wise Feed-Forward Network**가 사실상 MLP 구조  
    (차이: 각 토큰 위치별로 동일 가중치를 적용해 독립적으로 처리)

---

### 자연어 처리의 주요 기법 및 모델

#### 1. TF-IDF (Term Frequency - Inverse Document Frequency)
- **원리**: 단어의 문서 내 등장 빈도(TF) × 역문서 빈도(IDF)  
  → 자주 등장하는 단어 가중치↓, 희귀 단어 가중치↑
- **장점**: 단순하고 빠름, 적은 데이터에도 적용 가능
- **단점**: 단어 순서와 문맥 정보 손실, 유사어 처리 불가

#### 2. Word2Vec
- **원리**: 단어를 벡터 공간에 매핑, 유사한 의미의 단어가 가까운 벡터 위치에 놓이도록 학습
- **구조**:
    - **CBOW**: 주변 단어로 중심 단어 예측
    - **Skip-gram**: 중심 단어로 주변 단어 예측
- **장점**: 단어 의미 반영, 유사 단어 검색 가능
- **단점**: 문맥별 의미 변화 반영 불가 (정적 임베딩)

#### 3. BERT (Bidirectional Encoder Representations from Transformers)
- **원리**: Transformer 인코더 기반, 문장을 양방향으로 읽어 문맥을 깊게 이해
- **학습 방식**: Masked Language Modeling(MLM) + Next Sentence Prediction(NSP)
- **장점**: 문맥 이해력 뛰어남, 다양한 NLP 태스크에서 높은 성능
- **단점**: 모델 크고 연산량 많음

#### 4. GPT (Generative Pretrained Transformer)
- **원리**: Transformer 디코더 기반, 다음 단어 예측(Autoregressive) 방식
- **장점**: 자연스러운 문장 생성, 문맥 유지력 우수
- **단점**: 생성 제어 어려움, 사실 검증 필요

---

### 트랜스포머 구조 상세
트랜스포머는 인코더-디코더 구조이며, 각 블록의 핵심 구성 요소는 다음과 같음

#### Multi-Head Self-Attention
- 입력 토큰 간의 관계를 여러 개의 어텐션 헤드에서 병렬로 학습
- Query, Key, Value 벡터 연산 → 어텐션 스코어 → 소프트맥스 → 가중합

#### Position-Wise Feed-Forward Networks
- 각 토큰 벡터를 비선형적으로 변환하는 2층 MLP 구조
- 모든 토큰 위치에 동일한 가중치 적용, 위치별 독립 처리

#### Residual Connection and Layer Normalization
- 입력을 출력에 더해(Residual) 기울기 소실 완화
- Layer Normalization으로 학습 안정화 및 수렴 속도 향상

#### Positional Encoding
- 단어 순서 정보를 sine/cosine 파형 또는 학습형 벡터로 인코딩하여 입력에 더함

**처리 흐름**
1. **인코더**
    - 위치 인코딩 + 입력 임베딩
    - Multi-Head Self-Attention
    - Position-Wise Feed-Forward Networks
    - 잔차연결 + 층 정규화
    - 반복
2. **디코더**
    - 위치 인코딩 + 입력 임베딩
    - Masked Multi-Head Self-Attention (미래 토큰 차단)
    - Encoder-Decoder Attention
    - Position-Wise Feed-Forward Networks
3. **출력**
    - 소프트맥스로 다음 단어 확률 예측

---

## 📌 목적별 전처리 전략

### 1. **OCR 정확도 향상**
- 이진화: Otsu 또는 Adaptive Thresholding
- 노이즈 제거: Gaussian Blur, Median Filter, 모폴로지 연산
- 기울기 보정: Hough Transform으로 각도 검출 후 회전
- 해상도: 최소 300 DPI 이상 확보
- ROI 추출: 여백, 로고, 표 테두리 제거

### 2. **문장 분리 정확도 향상**
- 불필요 개행 제거
- 문장부호 복원 (`.`, `?`, `!` 누락 시 규칙 기반 복원)
- 숫자·단위 띄어쓰기 (`0.1mSv` → `0.1 mSv`)
- 한국어: KSS, Kiwi / 영어: NLTK, spaCy

### 3. **요약 품질 향상**
- 광고·메타데이터 제거 ("TEL", "FAX", "목차" 등)
- 중복 문장 제거 (Cosine Similarity, 해시)
- 단위 표기 통일 (`km`, `킬로미터` → `km`)
- 불필요 표·긴 숫자 나열 제거

### 4. **키워드 추출 정확도 향상**
- 불용어 제거 (조사, 접속사 등)
- 형태소 분석 후 명사·동사 원형 추출
- 복합 명사 유지 (`한국전력공사`)
- 특수문자·URL·이모지 삭제

### 5. **문서 분류 정확도 향상**
- 라벨 관련 핵심어 유지
- 동의어 통일 (`전력량` ↔ `전력소비량`)
- 문서 길이 균형화
- 숫자·단위 표준화

### 6. **개체명 인식(NER) 정확도 향상**
- 불필요 괄호/태그 삭제 (`[자료]`, `(사진)`)
- 날짜·수치 패턴 보존 (`2025-08-13`, `15.7 MW`)
- 영어 소문자 통일, 고유명사 예외 처리
- 띄어쓰기 보정 (PyKoSpacing, KIWI)

---

## 요약 기법
> 요약 품질을 높이기 위해서는 텍스트를 분할하고 의미 단위별로 요약하는 것이 중요함.

1. **Stuff 방식**  
   - 텍스트가 짧고 중요 맥락 유지가 필요할 때  
   - 전체 텍스트를 한 번에 LLM에 입력해 요약  
   - 장점: 구현 간단, 맥락 보존  
   - 단점: 입력 길이 제한, 토큰 비용 증가

2. **Map-Reduce 방식**  
   - 대규모 텍스트, 병렬 처리 필요 시  
   - 청크별 요약 → 요약 모음 재요약  
   - 장점: 확장성, 효율성  
   - 단점: 맥락 단절 가능, 최종 통합 어려움

3. **Refine 방식**  
   - 품질 중심, 정교한 요약 필요 시  
   - 초기 요약 → 다음 청크 추가 후 재요약 반복  
   - 장점: 맥락 반영, 유연한 통합  
   - 단점: 구현 복잡, 처리 시간 증가


# 디자인패턴
